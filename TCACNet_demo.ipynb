{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda !\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If a GPU is available, use it\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    use_cuda = True\n",
    "    print('Using cuda !')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    use_cuda = False\n",
    "    print('GPU not available !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, label, transform=None):\n",
    "        self.data = data\n",
    "        self.data_len = len(self.data)\n",
    "        self.label_arr = label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = int(self.label_arr[index])\n",
    "        sample = (self.data[index]).reshape(1, 44, 1126)\n",
    "        sample = torch.from_numpy(sample)\n",
    "        return sample, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "\n",
    "def EEGdata_loader():\n",
    "\n",
    "    x_train = np.load(r'.\\MIEEG data\\x_train.npy')\n",
    "    x_test = np.load(r'.\\MIEEG data\\x_test.npy')\n",
    "    x_valid = np.load(r'.\\MIEEG data\\x_valid.npy')\n",
    "    y_train = np.load(r'.\\MIEEG data\\y_train.npy')\n",
    "    y_test = np.load(r'.\\MIEEG data\\y_test.npy')\n",
    "    y_valid = np.load(r'.\\MIEEG data\\y_valid.npy')\n",
    "\n",
    "    train_data = CustomDataset(x_train, y_train)\n",
    "    valid_data = CustomDataset(x_valid, y_valid)\n",
    "    test_data = CustomDataset(x_test, y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, pin_memory=use_cuda, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_data, batch_size=batch_size_eval, pin_memory=use_cuda)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size_eval, pin_memory=use_cuda)\n",
    "    \n",
    "    return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tcacnet_utils.attention import inference\n",
    "\n",
    "def train(model_global, model_local, model_top, optimizer, loss_fn_local_top, epoch, only_global_model):\n",
    "    \n",
    "    model_global.train()\n",
    "    model_local.train()\n",
    "    model_top.train()\n",
    "\n",
    "    for batch_idx, (inputs, target) in enumerate(train_loader):\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        inputs = inputs.float()\n",
    "        target = target.long()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        wpser = inputs[:,:,:,-1]    # WPSER corresponding to each channel\n",
    "        inputs = inputs[:,:,:,0:inputs.shape[3]-1]    # raw EEG signal\n",
    "        output_merged, hint_loss, channel_loss = inference(inputs, wpser, model_global, model_local, model_top, n_slices, device,\n",
    "                                                           only_global_model, is_training=True)\n",
    "        \n",
    "        loss_local_and_top = loss_fn_local_top(output_merged, target)\n",
    "        loss_global_model = loss_local_and_top + hint_loss + channel_loss\n",
    "\n",
    "        for param in model_local.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model_top.parameters():\n",
    "            param.requires_grad = False\n",
    "        loss_global_model.backward(retain_graph=True)\n",
    "        for param in model_local.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in model_top.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in model_global.parameters():\n",
    "            param.requires_grad = False\n",
    "        loss_local_and_top.backward()\n",
    "        for param in model_global.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print('\\rTrain Epoch: {}'   \n",
    "              '  Total_Loss: {:.4f} (CrossEntropy: {:.2f} Hint: {:.2f} Ch: {:.2f})'\n",
    "              ''.format(epoch, loss_local_and_top.item()+hint_loss.item(), loss_local_and_top.item(), hint_loss.item(), channel_loss.item()),\n",
    "              end='')\n",
    "            \n",
    "    return loss_local_and_top.item()+hint_loss.item()+channel_loss.item(), loss_local_and_top.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model_global, model_local, model_top, test_loss_fn_local_top, epoch, loader, only_global_model):\n",
    "    model_global.eval()\n",
    "    model_local.eval()\n",
    "    model_top.eval()\n",
    "\n",
    "    avg_test_loss, avg_hint_loss, avg_channel_loss = 0, 0, 0\n",
    "    correct = 0\n",
    "    test_size = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, target in loader:\n",
    "            inputs, target = inputs.to(device), target.to(device)\n",
    "            \n",
    "            inputs = inputs.float()\n",
    "            target = target.long()\n",
    "\n",
    "            wpser = inputs[:,:,:,-1]\n",
    "            inputs = inputs[:,:,:,0:inputs.shape[3]-1]\n",
    "            \n",
    "            output_merged, hint_loss, channel_loss = inference(inputs, wpser, model_global, model_local, model_top, n_slices, device,\n",
    "                                                               only_global_model, is_training=False)\n",
    "\n",
    "            test_size += len(inputs)\n",
    "            avg_test_loss += test_loss_fn_local_top(output_merged, target).item()\n",
    "            avg_hint_loss += len(inputs) * hint_loss.item()\n",
    "            avg_channel_loss += len(inputs) * channel_loss.item()\n",
    "            pred = output_merged.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    avg_test_loss /= test_size\n",
    "    avg_hint_loss /= test_size\n",
    "    avg_channel_loss /= test_size\n",
    "    accuracy = correct / test_size\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print('\\nTest set: Avg_Total_Loss: {:.4f} (CrossEntropy: {:.2f} Hint: {:.2f} Ch: {:.2f})' \n",
    "              '  Accuracy: {}/{} ({:.0f}%)\\n'\n",
    "              .format(avg_test_loss + avg_hint_loss + avg_channel_loss, avg_test_loss, avg_hint_loss, avg_channel_loss,\n",
    "                      correct, test_size, 100. * accuracy))\n",
    "\n",
    "    return avg_test_loss+avg_hint_loss+avg_channel_loss, avg_test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from braindecode.torch_ext.optimizers import AdamW\n",
    "from tcacnet_utils.network import globalnetwork, localnetwork, topnetwork\n",
    "\n",
    "n_slices = 1    # number of time slices\n",
    "\n",
    "n_epochs = 200\n",
    "loss_fn_local_top = nn.NLLLoss()\n",
    "test_loss_fn_local_top = nn.NLLLoss(reduction='sum')\n",
    "learning_rate = 0.0625 * 0.01\n",
    "batch_size = 16\n",
    "batch_size_eval = 16\n",
    "\n",
    "train_loader, valid_loader, test_loader = EEGdata_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0  Total_Loss: 0.9760 (CrossEntropy: 0.98 Hint: 0.00 Ch: 0.00)\n",
      "Test set: Avg_Total_Loss: 0.6309 (CrossEntropy: 0.63 Hint: 0.00 Ch: 0.00)  Accuracy: 124/163 (76%)\n",
      "\n",
      "Train Epoch: 10  Total_Loss: 0.0413 (CrossEntropy: 0.04 Hint: 0.00 Ch: 0.00)\n",
      "Test set: Avg_Total_Loss: 0.2797 (CrossEntropy: 0.28 Hint: 0.00 Ch: 0.00)  Accuracy: 146/163 (90%)\n",
      "\n",
      "Train Epoch: 20  Total_Loss: 0.0292 (CrossEntropy: 0.03 Hint: 0.00 Ch: 0.00)\n",
      "Test set: Avg_Total_Loss: 0.2589 (CrossEntropy: 0.26 Hint: 0.00 Ch: 0.00)  Accuracy: 147/163 (90%)\n",
      "\n",
      "Train Epoch: 30  Total_Loss: 0.0042 (CrossEntropy: 0.00 Hint: 0.00 Ch: 0.00)\n",
      "Test set: Avg_Total_Loss: 0.2649 (CrossEntropy: 0.26 Hint: 0.00 Ch: 0.00)  Accuracy: 146/163 (90%)\n",
      "\n",
      "Train Epoch: 40  Total_Loss: 0.0180 (CrossEntropy: 0.02 Hint: 0.00 Ch: 0.00)\n",
      "Test set: Avg_Total_Loss: 0.3003 (CrossEntropy: 0.30 Hint: 0.00 Ch: 0.00)  Accuracy: 147/163 (90%)\n",
      "\n",
      "Train Epoch: 50  Total_Loss: 0.0012 (CrossEntropy: 0.00 Hint: 0.00 Ch: 0.00)\n",
      "Test set: Avg_Total_Loss: 0.2721 (CrossEntropy: 0.27 Hint: 0.00 Ch: 0.00)  Accuracy: 150/163 (92%)\n",
      "\n",
      "Train Epoch: 60  Total_Loss: 0.0011 (CrossEntropy: 0.00 Hint: 0.00 Ch: 0.00)\n",
      "Test set: Avg_Total_Loss: 0.2862 (CrossEntropy: 0.29 Hint: 0.00 Ch: 0.00)  Accuracy: 150/163 (92%)\n",
      "\n",
      "Train Epoch: 70  Total_Loss: 0.0007 (CrossEntropy: 0.00 Hint: 0.00 Ch: 0.00)\n",
      "Test set: Avg_Total_Loss: 0.2913 (CrossEntropy: 0.29 Hint: 0.00 Ch: 0.00)  Accuracy: 149/163 (91%)\n",
      "\n",
      "Train Epoch: 80  Total_Loss: 0.0006 (CrossEntropy: 0.00 Hint: 0.00 Ch: 0.00)\n",
      "Test set: Avg_Total_Loss: 0.3494 (CrossEntropy: 0.35 Hint: 0.00 Ch: 0.00)  Accuracy: 145/163 (89%)\n",
      "\n",
      "Train Epoch: 90  Total_Loss: 0.0013 (CrossEntropy: 0.00 Hint: 0.00 Ch: 0.00)\n",
      "Test set: Avg_Total_Loss: 0.2546 (CrossEntropy: 0.25 Hint: 0.00 Ch: 0.00)  Accuracy: 151/163 (93%)\n",
      "\n",
      "Train Epoch: 100  Total_Loss: 0.0038 (CrossEntropy: 0.00 Hint: 0.00 Ch: 0.00)\n",
      "Test set: Avg_Total_Loss: 0.2867 (CrossEntropy: 0.29 Hint: 0.00 Ch: 0.00)  Accuracy: 148/163 (91%)\n",
      "\n",
      "Train Epoch: 110  Total_Loss: 0.0029 (CrossEntropy: 0.00 Hint: 0.00 Ch: 0.00)\n",
      "Test set: Avg_Total_Loss: 0.3428 (CrossEntropy: 0.34 Hint: 0.00 Ch: 0.00)  Accuracy: 146/163 (90%)\n",
      "\n",
      "Train Epoch: 120  Total_Loss: 0.0023 (CrossEntropy: 0.00 Hint: 0.00 Ch: 0.00)\n",
      "Test set: Avg_Total_Loss: 0.3616 (CrossEntropy: 0.36 Hint: 0.00 Ch: 0.00)  Accuracy: 144/163 (88%)\n",
      "\n",
      "Train Epoch: 130  Total_Loss: 0.0017 (CrossEntropy: 0.00 Hint: 0.00 Ch: 0.00)\n",
      "Test set: Avg_Total_Loss: 0.3285 (CrossEntropy: 0.33 Hint: 0.00 Ch: 0.00)  Accuracy: 149/163 (91%)\n",
      "\n",
      "Train Epoch: 140  Total_Loss: 0.0007 (CrossEntropy: 0.00 Hint: 0.00 Ch: 0.00)\n",
      "Test set: Avg_Total_Loss: 0.3601 (CrossEntropy: 0.36 Hint: 0.00 Ch: 0.00)  Accuracy: 145/163 (89%)\n",
      "\n",
      "Train Epoch: 150  Total_Loss: 0.0003 (CrossEntropy: 0.00 Hint: 0.00 Ch: 0.00)\n",
      "Test set: Avg_Total_Loss: 0.3696 (CrossEntropy: 0.37 Hint: 0.00 Ch: 0.00)  Accuracy: 147/163 (90%)\n",
      "\n",
      "Train Epoch: 160  Total_Loss: 0.0007 (CrossEntropy: 0.00 Hint: 0.00 Ch: 0.00)\n",
      "Test set: Avg_Total_Loss: 0.3808 (CrossEntropy: 0.38 Hint: 0.00 Ch: 0.00)  Accuracy: 146/163 (90%)\n",
      "\n",
      "Train Epoch: 170  Total_Loss: 0.0001 (CrossEntropy: 0.00 Hint: 0.00 Ch: 0.00)\n",
      "Test set: Avg_Total_Loss: 0.4031 (CrossEntropy: 0.40 Hint: 0.00 Ch: 0.00)  Accuracy: 145/163 (89%)\n",
      "\n",
      "Train Epoch: 180  Total_Loss: 0.0070 (CrossEntropy: 0.01 Hint: 0.00 Ch: 0.00)\n",
      "Test set: Avg_Total_Loss: 0.3807 (CrossEntropy: 0.38 Hint: 0.00 Ch: 0.00)  Accuracy: 146/163 (90%)\n",
      "\n",
      "Train Epoch: 190  Total_Loss: 0.0010 (CrossEntropy: 0.00 Hint: 0.00 Ch: 0.00)\n",
      "Test set: Avg_Total_Loss: 0.3959 (CrossEntropy: 0.40 Hint: 0.00 Ch: 0.00)  Accuracy: 146/163 (90%)\n",
      "\n",
      "\n",
      "Use global model:\n",
      "\n",
      "Test set: Avg_Total_Loss: 0.4556 (CrossEntropy: 0.46 Hint: 0.00 Ch: 0.00)  Accuracy: 134/160 (84%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_global = globalnetwork().to(device)\n",
    "model_local = localnetwork().to(device)\n",
    "model_top = topnetwork().to(device)\n",
    "\n",
    "only_global_model = True    # only use global model\n",
    "\n",
    "if only_global_model:\n",
    "    optimizer = optim.Adam(list(model_global.parameters())\n",
    "                           + list(model_top.parameters()), lr=learning_rate)\n",
    "else:\n",
    "    optimizer = optim.Adam(list(model_global.parameters())\n",
    "                           + list(model_local.parameters())\n",
    "                           + list(model_top.parameters()), lr=learning_rate)\n",
    "\n",
    "min_cross_entropy = 100000\n",
    "\n",
    "for ep in range(n_epochs):\n",
    "    train_total_loss, train_cross_entropy = train(model_global, model_local, model_top, optimizer,\n",
    "                                                  loss_fn_local_top, ep, only_global_model)\n",
    "    valid_total_loss, valid_cross_entropy, valid_acc = test(model_global, model_local, model_top,\n",
    "                                                            test_loss_fn_local_top, ep, valid_loader, only_global_model)\n",
    "    if valid_cross_entropy < min_cross_entropy:\n",
    "        min_cross_entropy = valid_cross_entropy\n",
    "        torch.save(model_global.state_dict(),'model_global_cross_entropy.pth')\n",
    "        torch.save(model_local.state_dict(),'model_local_cross_entropy.pth')\n",
    "        torch.save(model_top.state_dict(),'model_top_cross_entropy.pth')\n",
    "\n",
    "if only_global_model:\n",
    "    print('\\nUse global model:')\n",
    "else:\n",
    "    print('\\nUse TCACNet:')       \n",
    "\n",
    "model_global.load_state_dict(torch.load('model_global_cross_entropy.pth'))\n",
    "model_local.load_state_dict(torch.load('model_local_cross_entropy.pth'))\n",
    "model_top.load_state_dict(torch.load('model_top_cross_entropy.pth'))\n",
    "valid_total_loss, valid_cross_entropy, valid_acc = test(model_global, model_local, model_top,\n",
    "                                                        test_loss_fn_local_top, 0, test_loader, only_global_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\jupyter notebook\\0530\\tcacnet_utils\\attention.py:18: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  idx_x = indexes // feature_w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0  Total_Loss: 2.9238 (CrossEntropy: 0.59 Hint: 2.33 Ch: 7.51)\n",
      "Test set: Avg_Total_Loss: 9.7704 (CrossEntropy: 0.90 Hint: 2.45 Ch: 6.42)  Accuracy: 97/163 (60%)\n",
      "\n",
      "Train Epoch: 10  Total_Loss: 1.9977 (CrossEntropy: 0.29 Hint: 1.71 Ch: 0.78)\n",
      "Test set: Avg_Total_Loss: 3.0804 (CrossEntropy: 0.37 Hint: 2.14 Ch: 0.57)  Accuracy: 147/163 (90%)\n",
      "\n",
      "Train Epoch: 20  Total_Loss: 1.7589 (CrossEntropy: 0.12 Hint: 1.64 Ch: 0.51)\n",
      "Test set: Avg_Total_Loss: 2.9569 (CrossEntropy: 0.31 Hint: 2.15 Ch: 0.50)  Accuracy: 147/163 (90%)\n",
      "\n",
      "Train Epoch: 30  Total_Loss: 1.7397 (CrossEntropy: 0.13 Hint: 1.61 Ch: 0.44)\n",
      "Test set: Avg_Total_Loss: 2.9098 (CrossEntropy: 0.34 Hint: 2.13 Ch: 0.44)  Accuracy: 147/163 (90%)\n",
      "\n",
      "Train Epoch: 40  Total_Loss: 2.1001 (CrossEntropy: 0.13 Hint: 1.97 Ch: 0.50)\n",
      "Test set: Avg_Total_Loss: 2.9528 (CrossEntropy: 0.34 Hint: 2.18 Ch: 0.43)  Accuracy: 144/163 (88%)\n",
      "\n",
      "Train Epoch: 50  Total_Loss: 2.0455 (CrossEntropy: 0.20 Hint: 1.85 Ch: 0.81)\n",
      "Test set: Avg_Total_Loss: 2.9072 (CrossEntropy: 0.29 Hint: 2.15 Ch: 0.46)  Accuracy: 144/163 (88%)\n",
      "\n",
      "Train Epoch: 60  Total_Loss: 1.9179 (CrossEntropy: 0.06 Hint: 1.85 Ch: 0.45)\n",
      "Test set: Avg_Total_Loss: 3.1204 (CrossEntropy: 0.40 Hint: 2.29 Ch: 0.43)  Accuracy: 136/163 (83%)\n",
      "\n",
      "Train Epoch: 70  Total_Loss: 2.1407 (CrossEntropy: 0.13 Hint: 2.01 Ch: 4.35)\n",
      "Test set: Avg_Total_Loss: 2.9866 (CrossEntropy: 0.32 Hint: 2.21 Ch: 0.45)  Accuracy: 142/163 (87%)\n",
      "\n",
      "Train Epoch: 80  Total_Loss: 1.8212 (CrossEntropy: 0.03 Hint: 1.79 Ch: 0.48)\n",
      "Test set: Avg_Total_Loss: 2.8810 (CrossEntropy: 0.32 Hint: 2.18 Ch: 0.38)  Accuracy: 145/163 (89%)\n",
      "\n",
      "Train Epoch: 90  Total_Loss: 2.0595 (CrossEntropy: 0.02 Hint: 2.04 Ch: 0.37)\n",
      "Test set: Avg_Total_Loss: 2.7195 (CrossEntropy: 0.28 Hint: 2.08 Ch: 0.36)  Accuracy: 146/163 (90%)\n",
      "\n",
      "Train Epoch: 100  Total_Loss: 1.9047 (CrossEntropy: 0.04 Hint: 1.86 Ch: 0.43)\n",
      "Test set: Avg_Total_Loss: 2.7504 (CrossEntropy: 0.32 Hint: 2.04 Ch: 0.39)  Accuracy: 147/163 (90%)\n",
      "\n",
      "Train Epoch: 110  Total_Loss: 1.9422 (CrossEntropy: 0.02 Hint: 1.92 Ch: 0.46)\n",
      "Test set: Avg_Total_Loss: 2.8100 (CrossEntropy: 0.40 Hint: 2.04 Ch: 0.37)  Accuracy: 144/163 (88%)\n",
      "\n",
      "Train Epoch: 120  Total_Loss: 2.1584 (CrossEntropy: 0.01 Hint: 2.14 Ch: 1.15)\n",
      "Test set: Avg_Total_Loss: 2.7406 (CrossEntropy: 0.32 Hint: 2.06 Ch: 0.36)  Accuracy: 142/163 (87%)\n",
      "\n",
      "Train Epoch: 130  Total_Loss: 2.0074 (CrossEntropy: 0.02 Hint: 1.99 Ch: 0.50)\n",
      "Test set: Avg_Total_Loss: 3.1253 (CrossEntropy: 0.52 Hint: 2.17 Ch: 0.44)  Accuracy: 135/163 (83%)\n",
      "\n",
      "Train Epoch: 140  Total_Loss: 1.7390 (CrossEntropy: 0.10 Hint: 1.64 Ch: 0.38)\n",
      "Test set: Avg_Total_Loss: 2.8382 (CrossEntropy: 0.45 Hint: 2.05 Ch: 0.34)  Accuracy: 136/163 (83%)\n",
      "\n",
      "Train Epoch: 150  Total_Loss: 2.2600 (CrossEntropy: 0.13 Hint: 2.13 Ch: 0.48)\n",
      "Test set: Avg_Total_Loss: 2.7803 (CrossEntropy: 0.38 Hint: 2.03 Ch: 0.38)  Accuracy: 142/163 (87%)\n",
      "\n",
      "Train Epoch: 160  Total_Loss: 2.0761 (CrossEntropy: 0.09 Hint: 1.99 Ch: 0.29)\n",
      "Test set: Avg_Total_Loss: 2.7362 (CrossEntropy: 0.37 Hint: 2.07 Ch: 0.30)  Accuracy: 144/163 (88%)\n",
      "\n",
      "Train Epoch: 170  Total_Loss: 2.0484 (CrossEntropy: 0.01 Hint: 2.04 Ch: 0.32)\n",
      "Test set: Avg_Total_Loss: 2.6416 (CrossEntropy: 0.32 Hint: 2.03 Ch: 0.28)  Accuracy: 145/163 (89%)\n",
      "\n",
      "Train Epoch: 180  Total_Loss: 2.1902 (CrossEntropy: 0.04 Hint: 2.15 Ch: 0.27)\n",
      "Test set: Avg_Total_Loss: 2.8945 (CrossEntropy: 0.46 Hint: 2.13 Ch: 0.30)  Accuracy: 143/163 (88%)\n",
      "\n",
      "Train Epoch: 190  Total_Loss: 1.5498 (CrossEntropy: 0.03 Hint: 1.52 Ch: 0.38)\n",
      "Test set: Avg_Total_Loss: 2.6381 (CrossEntropy: 0.33 Hint: 2.05 Ch: 0.26)  Accuracy: 147/163 (90%)\n",
      "\n",
      "\n",
      "Use TCACNet:\n",
      "\n",
      "Test set: Avg_Total_Loss: 3.0938 (CrossEntropy: 0.29 Hint: 2.19 Ch: 0.61)  Accuracy: 145/160 (91%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_global = globalnetwork().to(device)\n",
    "model_local = localnetwork().to(device)\n",
    "model_top = topnetwork().to(device)\n",
    "\n",
    "only_global_model = False    # use TCACNet\n",
    "\n",
    "if only_global_model:\n",
    "    optimizer = optim.Adam(list(model_global.parameters())\n",
    "                           + list(model_top.parameters()), lr=learning_rate)\n",
    "else:\n",
    "    optimizer = optim.Adam(list(model_global.parameters())\n",
    "                           + list(model_local.parameters())\n",
    "                           + list(model_top.parameters()), lr=learning_rate)\n",
    "\n",
    "min_cross_entropy = 100000\n",
    "for ep in range(n_epochs):\n",
    "    train_total_loss, train_cross_entropy = train(model_global, model_local, model_top, optimizer,\n",
    "                                                  loss_fn_local_top, ep, only_global_model)\n",
    "    valid_total_loss, valid_cross_entropy, valid_acc = test(model_global, model_local, model_top,\n",
    "                                                            test_loss_fn_local_top, ep, valid_loader, only_global_model)\n",
    "    if valid_cross_entropy < min_cross_entropy:\n",
    "        min_cross_entropy = valid_cross_entropy\n",
    "        torch.save(model_global.state_dict(),'model_global_cross_entropy.pth')\n",
    "        torch.save(model_local.state_dict(),'model_local_cross_entropy.pth')\n",
    "        torch.save(model_top.state_dict(),'model_top_cross_entropy.pth')\n",
    "        \n",
    "if only_global_model:\n",
    "    print('\\nUse global model:')\n",
    "else:\n",
    "    print('\\nUse TCACNet:')\n",
    "\n",
    "model_global.load_state_dict(torch.load('model_global_cross_entropy.pth'))\n",
    "model_local.load_state_dict(torch.load('model_local_cross_entropy.pth'))\n",
    "model_top.load_state_dict(torch.load('model_top_cross_entropy.pth'))\n",
    "valid_total_loss, valid_cross_entropy, valid_acc = test(model_global, model_local, model_top,\n",
    "                                                        test_loss_fn_local_top, 0, test_loader, only_global_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
